版本说明
Hadoop 50070
HBase 16010
Zookeeper 4181

名称 端口号
Hadoop Namenode Web UI 50070
Hadoop Datanode   Web UI 50075
Hadoop SecondaryNamenode Web UI 9001
HDFS 9000
Yarn Web UI 8088
Spark Web UI 8091
HBase Web UI 16010
Zookeeper 2888 3888 4181

#------------------------------------------------------------------------------------------------
# 构建Hadoop yarn spark Dockerfile文件内容
# Author:ywq

#基于centos7-ssh构建
FROM centos7-ssh

#配置各节点时间同步
RUN yum install -y ntp
#RUN systemctl is-enabled ntpd
#必须在run 容器时授权--privileged
#RUN systemctl enable ntpd
#RUN systemctl start ntpd
#docker容器与宿主机时区同步
RUN cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime && echo "Asia/shanghai" > /etc/timezone

#创建spark账号
RUN useradd spark
RUN echo "spark:12345678" | chpasswd

#对于Hbase 修改 ulimit 限制
RUN echo "spark  -      nofile  32768 " >> /etc/security/limits.conf
RUN echo "spark  -      nproc   32000" >>  /etc/security/limits.conf
RUN echo "session required pam_limits.so" >>  /etc/pam.d/common-session 

#安装java
ADD jdk-8u101-linux-x64.tar.gz /usr/local/
RUN mv /usr/local/jdk1.8.0_101 /usr/local/jdk1.8

#配置JAVA环境变量
ENV JAVA_HOME /usr/local/jdk1.8
ENV PATH $JAVA_HOME/bin:$PATH


#安装hadoop
ADD hadoop-2.7.3.tar.gz /usr/local
RUN mv /usr/local/hadoop-2.7.3 /usr/local/hadoop

#配置hadoop环境变量
ENV HADOOP_HOME /usr/local/hadoop
ENV PATH $HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH

#安装scala 注意Spark2.0.1对于Scala的版本要求是2.11.x
ADD scala-2.11.8.tgz /usr/local
RUN mv /usr/local/scala-2.11.8 /usr/local/scala2.11.8

#配置scala环境变量
ENV SCALA_HOME /usr/local/scala2.11.8
ENV PATH $SCALA_HOME/bin:$PATH

#安装spark
ADD spark-2.0.1-bin-hadoop2.7.tgz /usr/local
RUN mv /usr/local/spark-2.0.1-bin-hadoop2.7 /usr/local/spark2.0.1

#配置spark环境变量
ENV SPARK_HOME /usr/local/spark2.0.1
ENV PATH $SPARK_HOME/bin:$PATH

#安装ZooKeeper
ADD zookeeper-3.4.10.tar.gz /usr/local
RUN mv /usr/local/zookeeper-3.4.10 /usr/local/zookeeper3.4.10

#配置ZooKeeper环境变量
ENV ZOOKEEPERE_HOME /usr/local/zookeeper3.4.10
ENV PATH $ZOOKEEPERE_HOME/bin:$PATH

#安装HBase
ADD hbase-1.2.5-bin.tar.gz /usr/local
RUN mv /usr/local/hbase-1.2.5 /usr/local/hbase1.2.5

#配置Hbase环境变量
ENV HBASE_HOME /usr/local/hbase1.2.5
ENV PATH $HBASE_HOME/bin:$PATH


# bigdata configurations hdfs hbase zookeeper spark so on
ADD conf/hdfs_conf/core-site.xml $HADOOP_HOME/etc/hadoop/core-site.xml
ADD conf/hdfs_conf/hdfs-site.xml $HADOOP_HOME/etc/hadoop/hdfs-site.xml
ADD conf/hdfs_conf/mapred-site.xml $HADOOP_HOME/etc/hadoop/mapred-site.xml
ADD conf/hdfs_conf/yarn-site.xml $HADOOP_HOME/etc/hadoop/yarn-site.xml
ADD conf/hdfs_conf/slaves $HADOOP_HOME/etc/hadoop/slaves

ADD conf/spark_conf/spark-env.sh $SPARK_HOME/conf/spark-env.sh
ADD conf/spark_conf/slaves $SPARK_HOME/conf/slaves

ADD conf/zookeeper_conf/zoo.cfg $ZOOKEEPERE_HOME/conf/zoo.cfg

ADD conf/hbase_conf/hbase-site.xml $HBASE_HOME/conf/hbase-site.xml
ADD conf/hbase_conf/regionservers $HBASE_HOME/conf/regionservers

RUN echo "export JAVA_HOME=/usr/local/jdk1.8" >> $HBASE_HOME/conf/hbase-env.sh
RUN echo "export HBASE_MANAGES_ZK=false" >> $HBASE_HOME/conf/hbase-env.sh

RUN echo "export JAVA_HOME=/usr/local/jdk1.8" >> $HADOOP_HOME/etc/hadoop/hadoop-env.sh


#更改hadoop和spark2.0.1目录所属用户
RUN chown -R spark:spark /usr/local/hadoop
RUN chown -R spark:spark /usr/local/spark2.0.1
RUN chown -R spark:spark /usr/local/hbase1.2.5
RUN chown -R spark:spark /usr/local/zookeeper3.4.10

RUN yum install -y which sudo
#------------------------------------------------------------------------------------------------
docker build -t="bigdata-cluster" .


#------------------------
docker run --privileged -d -P -p 50070:50070 -p 50075:50075 -p 8088:8088 -p 8091:8091 -p 16010:16010 -p 4181:4181 --name master -h master --add-host slave01:172.17.0.3 --add-host slave02:172.17.0.4 bigdata-cluster

docker run --privileged -d -P -p 50070:50070 -p 50075:50075 -p 8088:8088 -p 8090:8080 -p 16010:16010 -p 4181:4181 --name master -h master --add-host slave01:172.17.0.3 --add-host slave02:172.17.0.4 bigdata-cluster
docker run --privileged -d -P  --name slave01 -h slave01 --add-host master:172.17.0.2 --add-host slave02:172.17.0.4 bigdata-cluster
docker run --privileged -d -P  --name slave02 -h slave02 --add-host master:172.17.0.2 --add-host slave01:172.17.0.3  bigdata-cluster
#------------------------


#------------------------
#生成spark账号的key，执行后会有多个输入提示，不用输入任何内容，全部直接回车即可
su spark
ssh-keygen
#拷贝到其他节点
ssh-copy-id -i /home/spark/.ssh/id_rsa -p 22 spark@master
ssh-copy-id -i /home/spark/.ssh/id_rsa -p 22 spark@slave01
ssh-copy-id -i /home/spark/.ssh/id_rsa -p 22 spark@slave02
#验证是否设置成功
ssh slave01
#------------------------


#------------------------
docker exec -it master bash
su spark
mkdir -p /usr/local/zookeeper3.4.10/data
echo "1" > /usr/local/zookeeper3.4.10/data/myid

docker exec -it slave01 bash
su spark
mkdir -p /usr/local/zookeeper3.4.10/data
echo "2" > /usr/local/zookeeper3.4.10/data/myid

docker exec -it slave02 bash
su spark
mkdir -p /usr/local/zookeeper3.4.10/data
echo "3" > /usr/local/zookeeper3.4.10/data/myid
#------------------------


#------------------------ su spark
#### 每个容器下执行
su spark
zkServer.sh start
#### 每个容器下执行

# 首次启动Hdfs，需要格式化
su spark
hdfs namenode -format

su spark
start-dfs.sh

#启动yarn
su spark
start-yarn.sh

#启动hbase
su spark
start-hbase.sh

#启动spark
su spark
sh /usr/local/spark2.0.1/sbin/start-all.sh
#------------------------


#--- 运行权限不足时
chown -R spark:spark /usr/local/hadoop
chown -R spark:spark /usr/local/spark2.0.1
chown -R spark:spark /usr/local/hbase1.2.5
# chown -R spark:spark /usr/local/zookeeper3.4.10
#--- 运行权限不足时


#------------------------
50070 Hadoop Namenode Web UI  ####
50075 Hadoop Datanode Web UI 
9001  Hadoop SecondaryNamenode Web UI ????
9000  HDFS 
8088 Yarn Web UI ####

8091 Spark Web UI !!!!
8090 = 8080 Spark Master Web UI $$$$
8091 = 8081 Spark Slave  Web UI $$$$

16010 HBase Web UI ####
2888 3888 4181 Zookeeper 2181 ####
#------------------------

32778
32779
32780 